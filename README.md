# Предсказание остановок оборудования с использованием LSTM и Байесовского подхода 

Данные были предоставлены ПАО «Северсталь» для [хакатона ЛИДЕРЫ ЦИФРОВОЙ ТРАНСФОРМАЦИИ 2023](https://leaders2023.innoagency.ru/task_15)

В проекте реaлизованы в коде идеи Давида Пагано (Davide Pagano) из статьи [A predictive maintenance model using Long Short-Term Memory Neural Networks and Bayesian inference](https://www.sciencedirect.com/science/article/pii/S2772662223000140#b7), опубликованные в Decision Analytics Journal в марте 2023. Из-за соглашения о неразглашении никакие подробности о компании, а также ее данные и какой-либо код для иллюстрации исследования автором в статье предоставлены не были.


## 1. Ресурсы
[Наборы обезличенных данных](https://drive.google.com/file/d/1jrbfHULbZuCnwJQwNllQUFlCGpR_lHDc/view?usp=sharing) из внутренних систем ПАО «Северсталь», содержащие:

* Перечень нештатных событий, приведших к остановке линии;
* Перечень нештатных событий, повлекших деградацию функций агрегатов;
* Перечень сигналов с датчиков, контроллеров.


## 2. Предварительная очистка данных

На этапе предварительной очистки данных переименовываются столбцы X_train и y_train, чтобы они были приемлемыми для pyspark. 
X_train очищается от отрицательных чисел, максимальная температура масла ограничивается 100 градусами, максимальная температура подшипников ограничивается 800 градусами.

Проводится передискретизация данных временных рядов с десяти секундного интервала до часового (берется среднее значение за час).
Передискретизированные данные сохраняются для дальнейшего использования.

Унифицируются названия технических мест. Унифицированные названия ("unified_name") и номер эксгаустера ("equipment") добавляются к перечню нештатных событий (messages).

[Ссылка на ноутбук](https://github.com/YaninaK/predictive-maintenance/blob/main/notebooks/01_Read_clean_and_resample_data.ipynb)


## 3. EDA

[Ссылка на ноутбук](https://github.com/YaninaK/predictive-maintenance/blob/main/notebooks/02_EDA.ipynb)

В messages фиксируются 2 вида нештатных событий: 

* M1 - остановки эксгаустеров
* M3 - аномалии в работе эксгаустеров

Всего 83 остановки (M1) и 898 периодов аномалий (M3). 

### 3.1. Остановки

Из 83 остановок только 72 - внеплановые, только их имеет смысл прогнозировать.

* Информация о внеплановых остановках доступна только по 14 техническим местам из 51. 
* Больше половины остановок произошло из-за неполадок с ротором, задвижкой и электродвигателем. 
* Причины остановок между эксгаустерами существенно отличаются.
* Наибольшее число остановок на четвертом и пятом эксгаустерах, наименьшее - на шестом и девятом.

### 3.2. Аномалии в работе эксгаустеров

* Больше половины всех аномалий зафиксировано в корпусах, роторах, редукторах, электродвигателях и задвижках эксгаустеров.
* Больше половины неполадок с корпусами приходилось на шестой и седьмой эксгаустеры.
* Больше половины неполадок с роторами произошли на седьмом и восьмом эксгаустерах.
* 40% аномалий в работе редукторов зафиксировано на четвертом эксгаустере.
* 32% аномалий в работе электродвигателей и 35% - в работе задвижек случились на пятом эксгаустере.

* У 130 аномалий из 898 нет данных об времени устранения неисправности.

### 3.3 Анализ разметки остановок и аномалий на y_train

* Разметка на y_train доступна менее чем по 60% технических мест.
* Аномалии работы двигателя резервного маслонасоса, эксгаустера Н-8000 (эксг 5) и регулирующей аппаратуры эксгаустера (эксг 7) не имеют разметки.
* Значительная часть разметки из messages не попала в y_train: 898 vs 395.

Из-за отсутствия значительной части разметки на y_train, разметка для модели создавалась на основе messages.


## 4. Модель
[Ссылка на ноутбук](https://github.com/YaninaK/predictive-maintenance/blob/main/notebooks/03_Baseline_model.ipynb)

### 4.1. Генерация эталонного датасета
[Ссылка на код](https://github.com/YaninaK/predictive-maintenance/blob/main/src/predictive_maintenance/features/etalon_periods.py)

#### Выбор эталонных периодов

В качестве эталонных периодов для обучения модели LSTM выбраны периоды с вибрацией ниже 10 и температурой ниже 80.

Все остановки и заданные периоды до и после остановок исключаются. 
* Период до остановки = длина входящей + длина исходящей последовательности LSTM модели (23 + 27 = 50 часов) 
* Период после остановки - 1 час.


#### Использование метода главных компонент (PCA) для генерации признаков

К нормализованным с помощью StandardScaler эталонным данным применен PCA с рассчитанным по правилу Кайзера числом компонент, равным 3, сгенерированы 3 латентные переменные, рассчитаны [Hotelling's T-Squared и Q residuals](https://wiki.eigenvector.com/index.php?title=T-Squared_Q_residuals_and_Contributions).

* Т2-статистика (Hotelling's T-Squared) — это мера вариации каждого образца в модели PCA, указывает, насколько далеко каждый образец находится из центра (оценки = 0) модели.

* Q-статистика (Q residuals) показывает, насколько хорошо каждый образец соответствует модели PCA. Это мера разницы или остатка между выборкой и ее проекцией на основные компоненты, сохраняемые в модели.

Таким образом, вместо данных с датчиков в эталонные периоды, используются 5 признаков, сгенерированных с помощью PCA.


#### Формирования датасета для модели LSTM

К полученному с помощью PCA дата фрейму, применено временное окно 50 (23 входящая последовательность + 27 исходящая).
Перед использованием в модели LSTM данные перетасовываются.

Размерность полученного датасета - (7062, 50, 5)

### 4.2. Обучение модели LSTM

На эталонном датасете обучается [модель LSTM](https://github.com/YaninaK/predictive-maintenance/blob/main/src/predictive_maintenance/models/model_LSTM.py) с двухслойными энкодером и декодером. 

В качестве функции потерь используется [Huber loss](https://en.wikipedia.org/wiki/Huber_loss).

Результаты инференса модели LSTM на эталонных данных подаются для обучения Байесовской модели.

### 4.3. Обучение Байесовской модели

Байесовская модель для обучения берет только два последних признака - предсказанные на эталонных данных T2 и Q-статистики.
Значение целевой функции у всех проставляем 1.

Модель должна выявлять отклонение от эталона внутри временного окна.


## 5. Инференс

### 5.1. LSTM – модель

Функция [get_M1_dataset](https://github.com/YaninaK/predictive-maintenance/blob/main/src/predictive_maintenance/models/stoppages.py) генерирует датасет для инференса LSTM – модели на текущих (не эталонных) данных.

* После загрузки данных переименовывает колонки, трансформирует данные, использует обученный StandardScaler и PCA-модель, рассчитывает Hotelling's T-Squared и Q residuals.
* Датасет формируется по данным о начале каждой остановки из messages (t): (t - 50 : t - 27). 

Таким образом, для инференса LSTM-модель получает датасет, размерностью (61, 23, 5)

Результат инференса LSTM-модели - датасет, размерностью (61, 27, 5).


### 5.2 Байесовская модель

Для инференса Байесовской модели  берутся только два последних признака из пяти - предсказанные на текущих (не эталонных) данных T2 и Q-статистики.
Ожидаемо, что T2 и Q эталонных  предсказаний должны существенно отличаться от предсказанных T2 и Q перед остановкой.

Байесовская модель предсказывает, как это отличие развивается во времени (в течение суток) с максимальным значением непосредственно перед остановкой.

Таким образом, о внеплановой остановке, модель предупреждает за сутки. 
На наших данных это происходит в каждом рассмотренном случае из доступных 61.
